# ============================================================================================
# DeepET - a deep learning framework for segmentation and classification of
#                  macromolecules in Cryo Electron Tomograms (Cryo-ET)
# ============================================================================================
# Copyright (c) 2021 - now
# ZIB - Department of Visual and Data Centric
# Author: Noushin Hajarolasvadi
# Team Leader: Daniel Baum
# License: GPL v3.0. See <https://www.gnu.org/licenses/>
# ============================================================================================
import re
import os
import time
import math
import shutil
import random
import string
from random import randrange
from utils.params import *
from utils.plots import *
from utils.utility_tools import *
from sklearn.metrics import confusion_matrix
from utils.CyclicLR.clr_callback import CyclicLR
from sklearn.metrics import precision_recall_fscore_support

from tensorflow import keras
from tensorflow.keras.optimizers import *
from tensorflow.keras import Model
from tensorflow.keras import layers
from tensorflow.keras import backend as bk
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint, ReduceLROnPlateau
import tensorflow as tf
from sklearn.model_selection import train_test_split

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

# loading batches of data
class DataPreparation(keras.utils.Sequence):
    """we iterate over the data as numpy arrays"""

    def __init__(self, obj):
        self.list_tomos_IDs = None
        self.list_masks_IDs = None
        # self.patches_tomos = []
        self.patches_masks = []
        self.batch_size = obj.batch_size
        self.patch_size = obj.patch_size
        self.img_size = obj.img_dim
        self.train_img_path = os.path.join(obj.base_path, "images/")
        self.train_target_path = os.path.join(obj.base_path, "targets/")

        seednum = 2
        np.random.seed(seednum)
        random_str = ''.join([random.choice(string.ascii_uppercase + string.digits) for n in range(16)])
        self.output_path = os.path.join(obj.output_path, random_str)
        os.makedirs(self.output_path)

        display("The output path is: ")
        display(self.output_path)

        # check values
        is_positive(self.batch_size, 'batch_size')
        is_positive(self.patch_size, 'patch_size')
        is_dir(self.train_img_path)
        is_dir(self.train_target_path)
        is_dir(self.output_path)

        # load the train and validation data
        self.fetch_dataset()

    def fetch_dataset(self):
        """ this functions generates a list of file names from
        all available tomograms in train_img_path fodler
        and their corresponding masks in train_target_path folder
        """
        display("fetching dataset...")
        from glob import glob
        self.list_tomos_IDs = glob(os.path.join(self.train_img_path, "*.mrc"))
        self.list_masks_IDs = glob(os.path.join(self.train_target_path, "*.mrc"))

        # check if every tomo has a corresponding mask
        if len(self.list_tomos_IDs) != len(self.list_masks_IDs):
            display("Expected two" + str(len(self.list_tomos_IDs)) + ", received " +
                   str(len(self.list_tomos_IDs)) + " and "+ str(len(self.list_masks_IDs)) +
                    ". \n There is a missing pair of tomogram and target.")
            sys.exit()

        self.list_tomos_IDs.sort(key=lambda f: int(re.sub('\D', '', f)))
        self.list_masks_IDs.sort(key=lambda r: int(re.sub('\D', '', r)))

        self.fetch_tomos()

    def fetch_tomos(self):
        """
        this function reads mrc files of tomograms and their corresponding masks from the list of files generated by
        fetch_dataset() function.
        Then it generates patches of size patch_size * patch_size * patch_size
        :return patches_tomos: patches generated from all tomograms in order
                patches_masks: patches generated from all masks in order
        """
        display("fetching tomograms...")
        start = time.clock()
        for t in range(len(self.list_tomos_IDs)):
            display("********** Fetch Tomo {tnum} **********".format(tnum=self.list_tomos_IDs[t]))
            self.tomo = read_mrc(self.list_tomos_IDs[t])
            self.mask = read_mrc(self.list_masks_IDs[t])

            # check if the tomogram and its mask are of the same size
            if self.tomo.shape != self.mask.shape:
                display("the tomogram and the target must be of the same size. " +
                        str(self.tomo.shape) + " is not equal to " + str(self.mask.shape) + ".")
                sys.exit()

            # preparation of tomogram as a tensor that can be used with tensorflow API
            self.tomo = np.swapaxes(self.tomo, 0, 2)  # changing dimension order from (z, y, x) to (x, y, z)
            self.tomo = np.expand_dims(self.tomo, axis=0)  # expanding dimensions for tensorflow input
            self.tomo = np.expand_dims(self.tomo, axis=4)  # expanding dimensions for tensorflow input

            # preparation of mask as a tensor that can be used with tensorflow API
            self.mask = np.swapaxes(self.mask, 0, 2)  # changing dimension order from (z, y, x) to (x, y, z)
            self.mask = np.expand_dims(self.mask, axis=0)  # expanding dimensions for tensorflow input
            self.mask = np.expand_dims(self.mask, axis=4)  # expanding dimensions for tensorflow input

            # extracting patches of size patch_size * patch_size * patch_size
            patch_tomo = tf.extract_volume_patches(self.tomo,
                                                   [1, self.patch_size, self.patch_size, self.patch_size, 1],
                                                   [1, self.patch_size, self.patch_size, self.patch_size, 1],
                                                   padding='VALID')
            patch_tomo = tf.reshape(patch_tomo, [-1, self.patch_size, self.patch_size, self.patch_size])
            patch_tomo = tf.squeeze(patch_tomo)

            # extracting patches of size patch_size * patch_size * patch_size
            patch_mask = tf.extract_volume_patches(self.mask,
                                                   [1, self.patch_size, self.patch_size, self.patch_size, 1],
                                                   [1, self.patch_size, self.patch_size, self.patch_size, 1],
                                                   padding='VALID')
            patch_mask = tf.reshape(patch_mask, [-1, self.patch_size, self.patch_size, self.patch_size])
            patch_mask = tf.squeeze(patch_mask)

            # converting back from tensor to numpy
            patch_tomo = patch_tomo.eval(session=tf.compat.v1.Session())
            patch_mask = patch_mask.eval(session=tf.compat.v1.Session())

            # the images are
            patch_tomo = np.swapaxes(patch_tomo, 1, 3)  # changing back dimension order from (x, y, z) to (z, y, x)
            patch_mask = np.swapaxes(patch_mask, 1, 3)  # changing back dimension order from (x, y, z) to (z, y, x)

            # concatenating all patches into a signle array (in order)
            if t == 0:
                self.patches_tomos = patch_tomo
                self.patches_masks = patch_mask
            else:
                self.patches_tomos = np.concatenate((self.patches_tomos, patch_tomo))
                self.patches_masks = np.concatenate((self.patches_masks, patch_mask))
            display("********** END Fetch Tomo {tnum} **********".format(tnum=self.list_tomos_IDs[t]))

        end = time.clock()
        process_time = (end - start)
        display("All tomograms fetched in {:.2f} seconds.".format(round(process_time, 2)))

class CNNModels:

    def __init__(self, obj):
        self.data = DataPreparation(obj)

        # define values
        self.net = None
        self.optimizer = None
        self.checkpoint = None
        self.layer_name = None
        self.process_time = None
        self.callbacks = None
        self.model_weight = None

        self.patch_overlap = False
        self.batch_idx = 0
        self.tomo_index = 0
        self.printstr = ""

        # values to collect outputs
        self.model_history = []
        self.history_lr = []
        self.batch_tomo = []
        self.batch_mask = []
        self.history_recall = []
        self.history_vald_acc = []
        self.history_f1_score = []
        self.history_train_acc = []
        self.history_vald_loss = []
        self.history_precision = []
        self.history_train_loss = []

        # initialize values
        self.obj = obj  # the object we receive from the form
        self.lr = self.obj.lr
        self.initial_lr = self.obj.lr
        self.width = self.obj.img_dim[0]
        self.height = self.obj.img_dim[1]
        if self.obj.dim_num > 2:
            self.depth = self.obj.img_dim[2]

        # calculating total number of patches of size patch_size * patch_size * patch_size
        # that can be extracted from one tomo
        self.nump_xaxis = int(np.floor(self.width / self.obj.patch_size))
        self.nump_yaxis = int(np.floor(self.height / self.obj.patch_size))
        self.nump_zaxis = int(np.floor(self.depth / self.obj.patch_size))

        self.nump_total = self.nump_xaxis * self.nump_yaxis * self.nump_zaxis

        # check values
        # TODO: please check other values as well. some has been already checked at the data preparation class
        is_positive(self.obj.epochs, 'epochs')
        is_positive(self.obj.classNum, 'num_class')

        # initialize model
        self.train_model()

    def train_model(self):
        """This function starts the training procedure by calling
           different built-in functions of the class CNNModel
        """
        self.get_model()  # build the CNN model
        self.fit_model()  # fit the data to the model and train the model
        self.plots()  # plot the results
        self.save()  # save the results
        plt.show(block=True)

    def get_model(self):
        if self.obj.model_type == "2D UNet":
            self.unet2d()
        elif self.obj.model_type == "3D UNet":
            self.unet3d()

        # set the properties of the mdoel
        self.set_optimizer()
        self.set_compile()
        print(self.net.summary())

    def fit_model(self):
        label_list = []
        for l in range(self.obj.classNum):
            label_list.append(l)
        start = time.clock()

        # if you use size of generated tensor it would be more accurate and it will never throw error
        steps_per_epoch = int(self.data.patches_tomos.shape[0] / self.obj.batch_size)
        counter = 0
        vald_batch_num = 0 # randrange(self.obj.epochs * steps_per_epoch)
        for e in range(self.obj.epochs):
            self.lr = self.initial_lr
            list_train_loss = []
            list_train_acc = []
            list_vald_acc = []
            list_vald_loss = []
            list_f1_score = []
            list_recall = []
            list_precision = []
            # self.realtime_output("########## Start Epoch {epochnum} ########## \n\n".format(epochnum=e))
            print("########## Start Epoch {epochnum} ##########".format(epochnum=e))
            # steps per epoch
            for b in range(steps_per_epoch):
                self.batch_idx = b
                # self.realtime_output("---------- Start Batch {bnum} ---------- \n\n".format(bnum=b))
                print("---------- Start Batch {bnum} ----------".format(bnum=b))

                # fetch the current batch of patches
                self.fetch_batch()

                # Split the data to train and validation (it shuffles the data so the order of patches is not the same )
                x_train, x_vald, y_train, y_vald = train_test_split(self.batch_tomo, self.batch_mask,
                                                                    test_size=0.2, shuffle=True)

                y_train = to_categorical(y_train, self.obj.classNum)
                y_vald = to_categorical(y_vald, self.obj.classNum)

                if counter == vald_batch_num:
                    self.batch_mask_vald = y_vald.reshape(y_vald.shape[0],
                                                          y_vald.shape[1] * y_vald.shape[2] *
                                                          y_vald.shape[3] * y_vald.shape[4])
                    print(self.batch_mask_vald.shape)

                # expanding dimensions to become suitable for the model input
                x_train = np.expand_dims(x_train, axis=4)
                x_vald = np.expand_dims(x_vald, axis=4)
                y_train = np.array(y_train)
                y_vald = np.array(y_vald)

                # train model on each batch
                # set learning schedule
                self.set_lr("exp_decay", counter)
                self.history_lr.append(self.lr)
                bk.set_value(self.net.optimizer.learning_rate, self.lr)  # set new learning_rate
                loss_train = self.net.train_on_batch(x_train, y_train, class_weight=self.model_weight)

                # ['loss', 'acc', 'precision', 'recall', 'auc']
                print("train loss: {tl}, train acc: {ta}".format(tl=loss_train[0], ta=loss_train[1]))
                list_train_loss.append(loss_train[0])
                list_train_acc.append(loss_train[1])

                # evaluate trained model on the validation set
                loss_val = self.net.evaluate(x_vald, y_vald, verbose=0)
                batch_pred = self.net.predict(x_vald)

                if counter == vald_batch_num:
                    self.vald_predicted_probs = batch_pred.reshape(batch_pred.shape[0],
                                                                   batch_pred.shape[1] * batch_pred.shape[2] *
                                                                   batch_pred.shape[3] * batch_pred.shape[4])
                    print(self.vald_predicted_probs.shape)
                    self.vald_predicted_labels = np.argmax(batch_pred, axis=1)

                scores = precision_recall_fscore_support(x_vald.argmax(axis=-1).flatten(),
                                                         batch_pred.argmax(axis=-1).flatten(), average=None,
                                                         labels=label_list, zero_division=0)
                print("val. loss: {vl}, val acc: {va}, f1 score: {f1s}".format(vl=loss_train[0],
                                                                               va=loss_train[1],
                                                                               f1s=loss_train[2]))
                list_vald_loss.append(loss_val[0])
                list_vald_acc.append(loss_val[1])
                list_f1_score.append(scores[2])
                list_recall.append(scores[1])
                list_precision.append(scores[0])
                # self.realtime_output("\n\n---------- END Batch {bnum} ---------- \n\n".format(bnum=b))
                print("---------- END Batch {bnum} ----------".format(bnum=b))
                counter = counter + 1

            self.history_train_loss.append(list_train_loss)
            self.history_train_acc.append(list_train_acc)
            self.history_vald_loss.append(list_vald_loss)
            self.history_vald_acc.append(list_vald_acc)
            self.history_f1_score.append(list_f1_score)
            self.history_recall.append(list_recall)
            self.history_precision.append(list_precision)

            # self.realtime_output("########## END Epoch {epochnum} ########## \n".format(epochnum=e))
            print("########## END Epoch {epochnum} ##########\n".format(epochnum=e))
        self.save_history()

        end = time.clock()
        self.process_time = (end - start)
        display(self.process_time)

    def fetch_batch(self):
        """
        this function fetches the patches from the current tomo based on the batch index
        """
        bstart = self.batch_idx * self.obj.batch_size
        bend = (self.batch_idx * self.obj.batch_size) + self.obj.batch_size

        self.batch_tomo = self.data.patches_tomos[bstart:bend]
        self.batch_mask = self.data.patches_masks[bstart:bend]

        # Patch base normalization
        self.batch_tomo = (self.batch_tomo - np.mean(self.batch_tomo)) / np.std(self.batch_tomo)
        # self.batch_mask_onehot = []
        # for m in range(len(self.batch_mask)):
        #     self.batch_mask_onehot.append(to_categorical(self.batch_mask[m], self.obj.classNum))

    def realtime_output(self, newstr):
        self.printstr = self.printstr + newstr
        self.obj.ui.textEdit.setText(self.printstr)

    def print_history(self, history):
        printstr = ""
        indxcol = 1
        for key, value in history.history.items():
            printstr = printstr + str(key) + " : " + str(value) + " | "
            if indxcol % 5 == 0:
                printstr = printstr + "\n"
            indxcol = indxcol + 1

        return printstr

    def save_history(self):
        # serialize model to JSON
        model_json = self.net.to_json()
        with open(os.path.join(self.data.output_path, "model.json"), "w") as json_file:
            json_file.write(model_json)

        save_csv(self.history_train_acc, self.data.output_path, "Train", "Accuracy_Details")
        save_csv(self.history_vald_acc, self.data.output_path, "Validation", "Accuracy_Details")
        save_csv(self.history_train_loss, self.data.output_path, "Train", "Loss_Details")
        save_csv(self.history_vald_loss, self.data.output_path, "Validation", "Loss_Details")
        save_csv(self.history_lr, self.data.output_path, "Train", "LearningRate_Details")
        save_csv(self.history_f1_score, self.data.output_path, "Validation", "F1Score_Details")
        save_csv(self.history_precision, self.data.output_path, "Validation", "Precision_Details")
        save_csv(self.history_recall, self.data.output_path, "Validation", "Recall_Details")

        # averaging the accuracy and loss over all folds
        self.train_acc = [np.mean([x[i] for x in self.history_train_acc]) for i in range(self.obj.epochs)]
        self.vald_acc = [np.mean([x[i] for x in self.history_vald_acc]) for i in range(self.obj.epochs)]
        self.train_loss = [np.mean([x[i] for x in self.history_train_loss]) for i in range(self.obj.epochs)]
        self.vald_loss = [np.mean([x[i] for x in self.history_vald_loss]) for i in range(self.obj.epochs)]
        self.f1_score = [np.mean([x[i] for x in self.history_f1_score]) for i in range(self.obj.epochs)]
        self.precision = [np.mean([x[i] for x in self.history_precision]) for i in range(self.obj.epochs)]
        self.recall = [np.mean([x[i] for x in self.history_recall]) for i in range(self.obj.epochs)]

        # saving the average results from folds
        save_csv(self.train_acc, self.data.output_path, flag="Train", name="Averaged_Accuracy")
        save_csv(self.vald_acc, self.data.output_path, flag="Validation", name="Averaged_Accuracy")
        save_csv(self.train_loss, self.data.output_path, flag="Train", name="Averaged_Loss")
        save_csv(self.vald_loss, self.data.output_path, flag="Validation", name="Averaged_Loss")
        save_csv(self.f1_score, self.data.output_path, flag="Validation", name="Averaged_F1")
        save_csv(self.precision, self.data.output_path, flag="Validation", name="Averaged_Precision")
        save_csv(self.recall, self.data.output_path, flag="Validation", name="Averaged_Recall")

    def plots(self):
        start_point = 0  # dropping the first few point in plots due to unstable behavior of model
        # cnf_matrix = np.zeros(shape=[self.obj.classNum, self.obj.classNum])

        plt.figure(num=1, figsize=(8, 6), dpi=100)
        plot_train_vs_vald(self.train_loss[start_point:], self.vald_loss[start_point:],
                           self.data.output_path, self.obj.epochs, is_loss=True)

        plt.figure(num=2, figsize=(8, 6), dpi=100)
        plot_train_vs_vald(self.train_acc[start_point:], self.vald_acc[start_point:],
                           self.data.output_path, self.obj.epochs)

        plt.figure(num=3, figsize=(8, 6), dpi=100)
        plot_lr(self.history_lr[start_point:], self.data.output_path, self.obj.epochs)

        general_plot(self.f1_score, ('F1-score', 'epochs'), self.obj.classNum, 4)
        general_plot(self.precision, ('Precision', 'epochs'), self.obj.classNum, 5)
        general_plot(self.recall, ('Recall', 'epochs'), self.obj.classNum, 6)

        # Plot all ROC curves
        plt.figure(num=7, figsize=(8, 6), dpi=100)
        plot_roc(self.batch_mask_vald, self.vald_predicted_probs,
                 self.obj.classNum, self.obj.output_path, self.obj.epochs)

        # Compute confusion matrix
        # cnf_matrix = confusion_matrix(self.batch_mask_vald, self.vald_predicted_labels)
        # np.set_printoptions(precision=2)

        # cnf_matrix2 = cnf_matrix.astype('float') / cnf_matrix.sum(axis=1)[:, np.newaxis]
        # print(np.average(cnf_matrix2.diagonal()))

        # # Plot and save non-normalized confusion matrix
        # plt.figure(num=5, figsize=(5, 5), dpi=100)
        # plot_confusion_matrix(cnf_matrix, self.obj.classNum, self.obj.output_path, self.obj.epochs)
        #
        # # Plot normalized confusion matrix
        # plt.figure(num=6, figsize=(5, 5), dpi=100)
        # plot_confusion_matrix(cnf_matrix, self.obj.classNum, self.obj.output_path, self.obj.epochs, normalize=True)
        #
        # cnf_matrix2 = cnf_matrix.astype('float') / cnf_matrix.sum(axis=1)[:, np.newaxis]
        # print(np.average(cnf_matrix2.diagonal()))

    def save(self):

        # evaluation on train
        # train_loss, train_acc, train_lr = self.net.evaluate(train_data, train_labels_one_hot_coded, batch_size=1)
        # print(train_loss, train_acc, train_lr)
        #
        # train_predicted_probs = self.net.predict(train_data, batch_size=1)
        # train_predicted_labels = train_predicted_probs.argmax(axis=-1)
        #
        # # Saving Train results
        # self.save_npy(train_predicted_probs, flag="Train", name="Probabilities")
        # self.save_npy(train_predicted_labels, flag="Train", name="ClassLabels")
        # self.save_csv(train_predicted_probs, flag="Train", name="Probabilities")
        # self.save_csv(train_predicted_labels, flag="Train", name="ClassLabels")
        #
        # # evaluation on Test
        # test_loss, test_acc, test_lr = self.net.evaluate(test_data, test_labels_one_hot_coded, batch_size=1)
        # print(test_loss, test_acc, test_lr)

        hyperparameter_setting = ""  # self.collect_results()
        with open(os.path.join(self.data.output_path, "HyperParameters.txt"), "w") as text_file:
            text_file.write(hyperparameter_setting)
        print(hyperparameter_setting)

        shutil.copyfile(os.path.join(ROOT_DIR, "code/models.py"),
                        os.path.join(self.data.output_path, "models.txt"))

    def set_optimizer(self):
        self.optimizer = Adam(lr=self.lr, beta_1=.9, beta_2=.999, epsilon=1e-08, decay=0.0)

        if self.obj.opt == "SGD":
            self.optimizer = SGD(lr=self.lr, decay=0.0, momentum=0.9, nesterov=True)
        elif self.obj.opt == "Adagrad":
            self.obj.optimizer = Adagrad(lr=self.lr, epsilon=1e-08, decay=0.0)
        elif self.obj.opt == "Adadelta":
            self.optimizer = Adadelta(lr=self.lr, rho=0.95, epsilon=1e-08, decay=0.0)
        elif self.obj.opt == "Adamax":
            self.optimizer = Adamax(lr=self.lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)
        elif self.obj.opt == "Nadam":
            self.optimizer = Nadam(lr=self.lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)
        elif self.obj.opt == "RMSprop":
            self.optimizer = RMSprop(lr=self.lr, rho=0.9, epsilon=1e-08, decay=0.0)

    # learning rate schedule
    def lr_decay_step(self, counter):
        self.lr = self.initial_lr * math.pow(.25, math.floor((1 + counter) / 2))

    def lr_decay_exp(self, counter):
        # compute the learning rate for the current epoch
        exp = np.floor((1 + counter) / 2)
        self.lr = self.initial_lr * (.25 ** exp)

    def lr_decay_poly(self, counter):
        decay = (1 - (counter / float(100))) ** 1.0
        self.lr = self.initial_lr * decay

    def set_lr(self, lr_type, counter):
        """schedule learning rate decay using different methods
        step Decay: drops learning rate by a factor as a step function
        exp decay: drops learning rate exponentially
        poly decay: drops learning rate by a polynomial function
        cyclic: drops learning rate by a cyclic method"""
        ReduceLROnPlateau(monitor='val_loss', factor=0.25, patience=10, min_lr=1e-06, mode='min', verbose=1)
        if lr_type == "step_decay":
            LearningRateScheduler(self.lr_decay_step(counter))
        elif lr_type == "exp_decay":
            LearningRateScheduler(self.lr_decay_exp(counter))
        elif lr_type == "poly_decay":
            LearningRateScheduler(self.lr_decay_poly(counter))
        elif lr_type == "cyclic":
            CyclicLR(base_lr=self.initial_lr, max_lr=6e-04, step_size=500., mode='exp_range', gamma=0.99994)

    def set_compile(self):
        # self.obj.metrics = ['accuracy', self.lr]
        # keras.metrics.TruePositives(name='tp'),
        # keras.metrics.FalsePositives(name='fp'),
        # keras.metrics.TrueNegatives(name='tn'),
        # keras.metrics.FalseNegatives(name='fn'),
        self.obj.metrics = [keras.metrics.BinaryAccuracy(name='acc'),
                            keras.metrics.Precision(name='precision'),
                            keras.metrics.Recall(name='recall'),
                            keras.metrics.AUC(name='auc')]
        if self.obj.loss != "tversky":
            self.net.compile(optimizer=self.optimizer, loss=self.obj.loss, metrics=[self.obj.metrics])
        else:
            self.net.compile(optimizer=self.optimizer, loss=self.tversky_loss, metrics=[self.obj.metrics])

    def set_checkpoint(self):
        checkpoint_dir = os.path.join(self.data.output_path,
                                      'weights-improvement-{epoch:03d}-{acc:.2f}-{loss:.2f}-{val_acc:.2f}-{val_loss:.2f}.hdf5')
        self.checkpoint = ModelCheckpoint(checkpoint_dir, monitor='val_acc', verbose=1, save_best_only=True, mode='max')

    def set_callback(self):
        # checkpoint results
        self.set_checkpoint()

        self.callbacks = [self.checkpoint]

    def save_layer_output(self, x, name="Train"):
        intermediate_layer_model = Model(inputs=self.net.input, outputs=self.net.get_layer(self.layer_name).output)
        intermediate_output = intermediate_layer_model.predict(x)
        filename = name + "_fc6_Layer_Features"
        np.save(os.path.join(self.data.output_path, filename), intermediate_output)

    def collect_results(self):
        # TODO: add calculation of union of interest in plots file.
        setting_info = "Saving folder Path =" + str(self.data.output_path)
        setting_info = setting_info + "\nData Path = " + str(self.data.train_img_path)
        setting_info = setting_info + "\nNumber of Epochs In Training = " + str(self.obj.epochs)
        setting_info = setting_info + "\nBatchsize = " + str(self.data.batch_size)
        setting_info = setting_info + "\nLearning Rate = " + str(self.lr)
        setting_info = setting_info + "\nFeatures Saved For Layer = " + str(self.layer_name)
        setting_info = setting_info + "\nCallbacks = " + self.callbacks
        setting_info = setting_info + "\nTrain accuracy = " + str(self.train_acc)
        setting_info = setting_info + "\nTrain loss = " + str(self.train_loss)
        setting_info = setting_info + "\nValidation accuracy = " + str(np.mean(self.vald_acc))
        setting_info = setting_info + "\nValidation loss = " + str(np.mean(self.vald_loss))
        setting_info = setting_info + "\nTest accuracy = " + str(self.test_acc)
        setting_info = setting_info + "\nTest loss = " + str(self.test_loss)
        setting_info = setting_info + "\nProcess Time in seconds = " + str(self.process_time)
        return setting_info

    def tversky_loss(self, y_true, y_pred):
        alpha = 0.5
        beta = 0.5

        ones = bk.ones(bk.shape(y_true))
        p0 = y_pred  # proba that voxels are class i
        p1 = ones - y_pred  # proba that voxels are not class i
        g0 = y_true
        g1 = ones - y_true

        num = bk.sum(p0 * g0, (0, 1, 2, 3))
        den = num + alpha * bk.sum(p0 * g1, (0, 1, 2, 3)) + beta * bk.sum(p1 * g0, (0, 1, 2, 3))

        t_sum = bk.sum(num / den)  # when summing over classes, T has dynamic range [0 Ncl]

        num_classes = bk.cast(bk.shape(y_true)[-1], 'float32')
        return num_classes - t_sum

    def unet2d(self):
        # The original 2D UNET mdoel
        input_img = layers.Input(shape=(self.width, self.height, 1))

        # down-sampling part of the network
        x = layers.Conv2D(32, 3, strides=2, padding="same")(input_img)
        x = layers.BatchNormalization()(x)
        x = layers.Activation("relu")(x)

        previous_block_activation = x  # Set aside residual

        # Blocks 1, 2, 3 are identical apart from the feature depth.
        for filters in [64, 128, 256]:
            x = layers.Activation("relu")(x)
            x = layers.SeparableConv2D(filters, 3, padding="same")(x)
            x = layers.BatchNormalization()(x)

            x = layers.Activation("relu")(x)
            x = layers.SeparableConv2D(filters, 3, padding="same")(x)
            x = layers.BatchNormalization()(x)

            x = layers.MaxPooling2D(3, strides=2, padding="same")(x)

            # Project residual
            residual = layers.Conv2D(filters, 1, strides=2, padding="same")(previous_block_activation)
            x = layers.add([x, residual])  # Add back residual
            previous_block_activation = x  # Set aside next residual

        # up-sampling part of the network
        for filters in [256, 128, 64, 32]:
            x = layers.Activation("relu")(x)
            x = layers.Conv2DTranspose(filters, 3, padding="same")(x)
            x = layers.BatchNormalization()(x)

            x = layers.Activation("relu")(x)
            x = layers.Conv2DTranspose(filters, 3, padding="same")(x)
            x = layers.BatchNormalization()(x)

            x = layers.UpSampling2D(2)(x)

            # Project residual
            residual = layers.UpSampling2D(2)(previous_block_activation)
            residual = layers.Conv2D(filters, 1, padding="same")(residual)
            x = layers.add([x, residual])  # Add back residual
            previous_block_activation = x  # Set aside next residual

        # Add a per-pixel classification layer
        outputs = layers.Conv2D(self.obj.classNum, 3, activation="softmax", padding="same")(x)

        # Define the model
        self.net = Model(input_img, outputs)

    def unet3d(self):
        # The UNET model from DeepFinder
        input_img = layers.Input(shape=(self.obj.patch_size, self.obj.patch_size, self.obj.patch_size, 1))

        x = layers.Conv3D(32, (3, 3, 3), padding='same', activation='relu')(input_img)
        high = layers.Conv3D(32, (3, 3, 3), padding='same', activation='relu')(x)

        x = layers.MaxPooling3D((2, 2, 2), strides=None)(high)

        x = layers.Conv3D(48, (3, 3, 3), padding='same', activation='relu')(x)
        mid = layers.Conv3D(48, (3, 3, 3), padding='same', activation='relu')(x)

        x = layers.MaxPooling3D((2, 2, 2), strides=None)(mid)

        x = layers.Conv3D(64, (3, 3, 3), padding='same', activation='relu')(x)
        x = layers.Conv3D(64, (3, 3, 3), padding='same', activation='relu')(x)
        x = layers.Conv3D(64, (3, 3, 3), padding='same', activation='relu')(x)
        x = layers.Conv3D(64, (3, 3, 3), padding='same', activation='relu')(x)

        x = layers.UpSampling3D(size=(2, 2, 2), data_format='channels_last')(x)
        x = layers.Conv3D(64, (2, 2, 2), padding='same', activation='relu')(x)

        x = layers.concatenate([x, mid])
        x = layers.Conv3D(48, (3, 3, 3), padding='same', activation='relu')(x)
        x = layers.Conv3D(48, (3, 3, 3), padding='same', activation='relu')(x)

        x = layers.UpSampling3D(size=(2, 2, 2), data_format='channels_last')(x)
        x = layers.Conv3D(48, (2, 2, 2), padding='same', activation='relu')(x)

        x = layers.concatenate([x, high])
        x = layers.Conv3D(32, (3, 3, 3), padding='same', activation='relu')(x)
        x = layers.Conv3D(32, (3, 3, 3), padding='same', activation='relu')(x)

        output = layers.Conv3D(self.obj.classNum, (1, 1, 1), padding='same', activation='softmax')(x)

        self.net = Model(input_img, output)
